{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9604823-e0b0-4072-871c-32391d80396b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import plot_importance\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e00e19-7a7a-4a45-b5b2-ffd8be34c9da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'D:/JuypterNotebook/Github/CasualXGBoost-for-TER/data/ForPCMCI/byPFT/'\n",
    "csv_list = os.listdir(file_path)\n",
    "csv_list = [f for f in csv_list if f[-4:] == '.csv']\n",
    "list_num = len(csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00453169-50d3-4a70-a7ef-470d0604603d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_dir = 'D:/JuypterNotebook/Github/CasualXGBoost-for-TER/data/XGBoost/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb85937-3497-4304-bad0-0928e223c67c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PFT</th>\n",
       "      <th>Air_temperature</th>\n",
       "      <th>Soil_temperature</th>\n",
       "      <th>VPD</th>\n",
       "      <th>SWC</th>\n",
       "      <th>Precipation</th>\n",
       "      <th>SOC_0</th>\n",
       "      <th>SOC_10</th>\n",
       "      <th>SOC_30</th>\n",
       "      <th>LAI</th>\n",
       "      <th>BS</th>\n",
       "      <th>PV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRO</td>\n",
       "      <td>0.095118</td>\n",
       "      <td>0.059092</td>\n",
       "      <td>-0.038036</td>\n",
       "      <td>0.054661</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.203047</td>\n",
       "      <td>-0.002347</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>-0.061759</td>\n",
       "      <td>-0.188539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DBF</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>-0.063205</td>\n",
       "      <td>-0.006156</td>\n",
       "      <td>0.020547</td>\n",
       "      <td>-0.011677</td>\n",
       "      <td>-0.037615</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>0.186071</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>-0.009181</td>\n",
       "      <td>-0.014210</td>\n",
       "      <td>-0.069288</td>\n",
       "      <td>-0.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EBF</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.052589</td>\n",
       "      <td>0.089852</td>\n",
       "      <td>-0.094596</td>\n",
       "      <td>-0.006911</td>\n",
       "      <td>-0.085589</td>\n",
       "      <td>0.054376</td>\n",
       "      <td>0.053910</td>\n",
       "      <td>0.082099</td>\n",
       "      <td>-0.054075</td>\n",
       "      <td>0.077295</td>\n",
       "      <td>-0.084768</td>\n",
       "      <td>-0.550733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENF</td>\n",
       "      <td>0.268388</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>-0.027481</td>\n",
       "      <td>0.013165</td>\n",
       "      <td>-0.012428</td>\n",
       "      <td>-0.018095</td>\n",
       "      <td>0.014095</td>\n",
       "      <td>-0.020663</td>\n",
       "      <td>-0.010045</td>\n",
       "      <td>-0.014592</td>\n",
       "      <td>-0.008571</td>\n",
       "      <td>-0.062217</td>\n",
       "      <td>0.018441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRA</td>\n",
       "      <td>0.042811</td>\n",
       "      <td>0.015401</td>\n",
       "      <td>-0.003516</td>\n",
       "      <td>0.042353</td>\n",
       "      <td>0.233636</td>\n",
       "      <td>0.027398</td>\n",
       "      <td>0.038403</td>\n",
       "      <td>-0.018373</td>\n",
       "      <td>0.364845</td>\n",
       "      <td>-0.023684</td>\n",
       "      <td>0.127569</td>\n",
       "      <td>-0.124530</td>\n",
       "      <td>-0.080207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MF.</td>\n",
       "      <td>0.176532</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>0.075196</td>\n",
       "      <td>0.072577</td>\n",
       "      <td>-0.046755</td>\n",
       "      <td>-0.039582</td>\n",
       "      <td>-0.037269</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.102972</td>\n",
       "      <td>0.042912</td>\n",
       "      <td>0.064225</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>-0.063409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SAV</td>\n",
       "      <td>0.080662</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>-0.016766</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.162545</td>\n",
       "      <td>-0.051205</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>-0.090611</td>\n",
       "      <td>0.135651</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>0.049577</td>\n",
       "      <td>-0.137008</td>\n",
       "      <td>-0.027794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SHR</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>-0.056226</td>\n",
       "      <td>-0.065191</td>\n",
       "      <td>-0.030609</td>\n",
       "      <td>0.265418</td>\n",
       "      <td>-0.034539</td>\n",
       "      <td>0.079018</td>\n",
       "      <td>0.080306</td>\n",
       "      <td>0.103123</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>-0.346964</td>\n",
       "      <td>-0.071965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PFT  Air_temperature  Soil_temperature       VPD       SWC  Precipation   \n",
       "0  CRO         0.095118          0.059092 -0.038036  0.054661    -0.004960  \\\n",
       "1  DBF         0.205556         -0.063205 -0.006156  0.020547    -0.011677   \n",
       "2  EBF         0.077930          0.052589  0.089852 -0.094596    -0.006911   \n",
       "3  ENF         0.268388          0.003773 -0.027481  0.013165    -0.012428   \n",
       "4  GRA         0.042811          0.015401 -0.003516  0.042353     0.233636   \n",
       "5  MF.         0.176532         -0.005934  0.075196  0.072577    -0.046755   \n",
       "6  SAV         0.080662          0.004045 -0.016766  0.019753     0.162545   \n",
       "7  SHR         0.010857         -0.056226 -0.065191 -0.030609     0.265418   \n",
       "\n",
       "      SOC_0    SOC_10    SOC_30       LAI        BS        PV       NPV   \n",
       "0  0.001132  0.001132  0.013933  0.203047 -0.002347  0.114800 -0.061759  \\\n",
       "1 -0.037615  0.006521  0.186071  0.090136 -0.009181 -0.014210 -0.069288   \n",
       "2 -0.085589  0.054376  0.053910  0.082099 -0.054075  0.077295 -0.084768   \n",
       "3 -0.018095  0.014095 -0.020663 -0.010045 -0.014592 -0.008571 -0.062217   \n",
       "4  0.027398  0.038403 -0.018373  0.364845 -0.023684  0.127569 -0.124530   \n",
       "5 -0.039582 -0.037269  0.007923  0.102972  0.042912  0.064225 -0.000769   \n",
       "6 -0.051205  0.063061 -0.090611  0.135651  0.046099  0.049577 -0.137008   \n",
       "7 -0.034539  0.079018  0.080306  0.103123  0.087029  0.063500 -0.346964   \n",
       "\n",
       "         CI  \n",
       "0 -0.188539  \n",
       "1 -0.005936  \n",
       "2 -0.550733  \n",
       "3  0.018441  \n",
       "4 -0.080207  \n",
       "5 -0.063409  \n",
       "6 -0.027794  \n",
       "7 -0.071965  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = pd.read_csv('D:/JuypterNotebook/Github/CasualXGBoost-for-TER/data/Causal_effect_PFT.csv')\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08dc9e30-0ea0-4d20-9059-1db428a48f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[837, 929, 321, 536, 710, 505, 213, 802, 671, 428, 839, 901, 683, 826, 277, 176, 399, 105, 776, 792]\n"
     ]
    }
   ],
   "source": [
    "# random_seeds = random.sample(range(1,1000),20)\n",
    "random_seeds = [837, 929, 321, 536, 710, 505, 213, 802, 671, 428, 839, 901, 683, 826, 277, 176, 399, 105, 776, 792]\n",
    "print(random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92d154d5-e7bc-47ca-97f9-6d3c0f5768e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_CRO.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_DBF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_EBF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_ENF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_GRA.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_MF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_SAV.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_SHR.csv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(list_num):\n",
    "    \n",
    "    file_name = file_path + csv_list[i]    \n",
    "    sheet_file = pd.read_csv(file_name)\n",
    "    pft = sheet_file.iloc[0,6]\n",
    "    print(file_name,'\\n')\n",
    "    \n",
    "    XGB_base_weight = pd.DataFrame()\n",
    "    XGB_causal_weight = pd.DataFrame()\n",
    "    \n",
    "    for j in range(len(random_seeds)):\n",
    "        \n",
    "        order = j+1\n",
    "        \n",
    "        sub_train = sheet_file.groupby('siteID').apply(pd.DataFrame.sample,\n",
    "                                                       frac = 0.8, random_state = random_seeds[j]).reset_index(level = 'siteID',drop = True)\n",
    "        sub_notra = sheet_file.drop(index=sub_train.index)\n",
    "        \n",
    "        x_train = sub_train.iloc[:,12:]\n",
    "        y_train = sub_train.iloc[:,11]\n",
    "        \n",
    "        x_test, x_val, y_test, y_val = train_test_split(sub_notra.iloc[:,12:], sub_notra.iloc[:,11],\n",
    "                                                        test_size=0.5,random_state=random_seeds[j])\n",
    "        \n",
    "        valid_set = [(x_train, y_train),(x_val, y_val)]\n",
    "        \n",
    "        model_base = xgb.XGBRegressor(n_estimators = 10000, early_stopping_rounds = 30,device = 'cpu',\n",
    "                                      learning_rate = 0.05, max_depth = 5, min_child_weight = 3,\n",
    "                                      gamma = 0.4, subsample = 0.8,\n",
    "                                      colsample_bynode = 0.8, colsample_bytree = 1, colsample_bylevel = 0.6,\n",
    "                                      random_state = 666\n",
    "                                     )\n",
    "\n",
    "        model_causal = xgb.XGBRegressor(n_estimators = 10000, early_stopping_rounds = 30,device = 'cpu',\n",
    "                                        learning_rate = 0.05, max_depth = 5, min_child_weight = 3,\n",
    "                                        gamma = 0.4, subsample = 0.8,\n",
    "                                        colsample_bynode = 0.8, colsample_bytree = 1, colsample_bylevel = 0.6,\n",
    "                                        random_state = 666\n",
    "                                       )\n",
    "        \n",
    "        XGB_Base = model_base.fit(x_train, y_train, eval_set = valid_set, verbose = False)\n",
    "        XGB_Causal = model_causal.fit(x_train, y_train, eval_set = valid_set, verbose = False,\n",
    "                                      feature_weights = abs(weight.iloc[i,1:].to_numpy()) )\n",
    "        \n",
    "        # weight save        \n",
    "        base_wight = pd.DataFrame([XGB_Base.get_booster().get_score(importance_type= 'weight')])\n",
    "        causal_wight = pd.DataFrame([XGB_Causal.get_booster().get_score(importance_type= 'weight')])\n",
    "        \n",
    "        XGB_base_weight = pd.concat([XGB_base_weight, base_wight])\n",
    "        XGB_causal_weight = pd.concat([XGB_causal_weight, causal_wight])\n",
    "        \n",
    "        # save learning curve\n",
    "        eval_base = XGB_Base.evals_result()\n",
    "        eval_causal = XGB_Causal.evals_result()\n",
    "        \n",
    "        loss_base = pd.DataFrame([eval_base['validation_0']['rmse'],eval_base['validation_1']['rmse']]).T\n",
    "        loss_causal = pd.DataFrame([eval_causal['validation_0']['rmse'],eval_causal['validation_1']['rmse']]).T\n",
    "        \n",
    "        loss_result = pd.concat([loss_base,loss_causal],axis=1)\n",
    "        loss_result.columns = ['base_train','base_valid','causal_train','causal_valid']\n",
    "        \n",
    "        loss_result.to_csv(out_dir+'Loss_curve/' + pft +'_loss_curve_'+ \"%02d\" %order +'_Seed_' + \"%03d\" %random_seeds[j] + '.csv',index = False )\n",
    "        \n",
    "        # save test predictions\n",
    "        TER_REF = y_test.reset_index(drop=True)\n",
    "        base_pred = pd.DataFrame(XGB_Base.predict(x_test)) \n",
    "        causal_pred = pd.DataFrame(XGB_Causal.predict(x_test))\n",
    "        \n",
    "        predicts = pd.concat([TER_REF,base_pred,causal_pred],axis=1)\n",
    "        predicts.columns = ['RECO_flux','RECO_base','RECO_causal']\n",
    "        \n",
    "        predicts.to_csv(out_dir+'Prediction/' + pft +'_Test_prediction_'+ \"%02d\" %order +'_Seed_' + \"%03d\" %random_seeds[j] + '.csv',index = False )\n",
    "        \n",
    "        # save model\n",
    "        joblib.dump(XGB_Base, out_dir + 'Model/Baseline_'+ pft + \"_%02d\" %order + '.joblib')\n",
    "        joblib.dump(XGB_Causal,  out_dir + 'Model/Causal_'+ pft + \"_%02d\" %order + '.joblib')\n",
    "    \n",
    "    # export xgb weight\n",
    "    XGB_base_weight.to_csv(out_dir + 'Weight/Baseline_'+ pft +'.csv',index = False)\n",
    "    XGB_causal_weight.to_csv(out_dir + 'Weight/Causal_'+ pft  +'.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f5989-bb61-4a29-9903-e0e69a12ef63",
   "metadata": {},
   "source": [
    "## leave one validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b617d1-b273-404b-8c09-cfa5d9839d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_CRO.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_DBF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_EBF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_ENF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_GRA.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_MF.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_SAV.csv \n",
      "\n",
      "D:/小论文_Paper(doing)/Paper_09_site scale causality guide XGBoost model for TER/Process/ForPCMCI/byPFT/sub_noNA_SHR.csv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(list_num):\n",
    "    \n",
    "    file_name = file_path + csv_list[i]    \n",
    "    sheet_file = pd.read_csv(file_name)\n",
    "    pft = sheet_file.iloc[0,6]\n",
    "    print(file_name,'\\n')\n",
    "    \n",
    "    site = np.unique(sheet_file['siteID'])\n",
    "    \n",
    "    for j in range(len(site)):\n",
    "        \n",
    "        siteID = site[j]\n",
    "        test = sheet_file[sheet_file['siteID'] == siteID]\n",
    "        train = sheet_file.drop(index = test.index)\n",
    "        \n",
    "        x_test = test.iloc[:,12:]\n",
    "        y_test = test.iloc[:,11]\n",
    "        \n",
    "        predicts_site = y_test.reset_index(drop=True)\n",
    "        \n",
    "        for k in range(20):\n",
    "            \n",
    "            order = k+1\n",
    "            \n",
    "            x_train, x_val, y_train, y_val = train_test_split(train.iloc[:,12:], train.iloc[:,11],\n",
    "                                                              test_size=0.8, random_state = random_seeds[k])\n",
    "            \n",
    "            valid_set = [(x_train, y_train),(x_val, y_val)]\n",
    "            \n",
    "            model_base = xgb.XGBRegressor(n_estimators = 10000, early_stopping_rounds = 30,device = 'cpu',\n",
    "                                          learning_rate = 0.05, max_depth = 5, min_child_weight = 3,\n",
    "                                          gamma = 0.4, subsample = 0.8,\n",
    "                                          colsample_bynode = 0.8, colsample_bytree = 1, colsample_bylevel = 0.6,\n",
    "                                          random_state = 666\n",
    "                                         )\n",
    "\n",
    "            model_causal = xgb.XGBRegressor(n_estimators = 10000, early_stopping_rounds = 30,device = 'cpu',\n",
    "                                            learning_rate = 0.05, max_depth = 5, min_child_weight = 3,\n",
    "                                            gamma = 0.4, subsample = 0.8,\n",
    "                                            colsample_bynode = 0.8, colsample_bytree = 1, colsample_bylevel = 0.6,\n",
    "                                            random_state = 666\n",
    "                                           )\n",
    "            \n",
    "            XGB_Base = model_base.fit(x_train, y_train, eval_set = valid_set, verbose = False)\n",
    "            XGB_Causal = model_causal.fit(x_train, y_train, eval_set = valid_set, verbose = False,\n",
    "                                          feature_weights = abs(weight.iloc[i,1:].to_numpy()) )\n",
    "            \n",
    "            # save prediction for each site\n",
    "            base_pred = pd.DataFrame(XGB_Base.predict(x_test)) \n",
    "            causal_pred = pd.DataFrame(XGB_Causal.predict(x_test))\n",
    "            \n",
    "            pred_times = pd.concat([base_pred,causal_pred],axis=1)\n",
    "            pred_times.columns = [\"RECO_base_\"+\"%02d\"%order,\"RECO_causal_\"+\"%02d\"%order]\n",
    "            \n",
    "            predicts_site = pd.concat([predicts_site,pred_times], axis=1)\n",
    "        \n",
    "        # export 20-round predictions for each site\n",
    "        predicts_site.to_csv(out_dir + 'LeaveOneCrossValidation/LeaveOne_' + pft +\"_SiteID_%03d\"%siteID + '.csv',index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a99af-9dec-428e-853d-ea5015cdd997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
